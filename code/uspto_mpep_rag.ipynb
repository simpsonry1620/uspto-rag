{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a17cc5a-962d-4179-bb73-801433b83ea7",
   "metadata": {},
   "source": [
    "# USPTO MPEP RAG Example (Step 1 document processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29e371f-5c09-4025-8b2b-ec71b7607696",
   "metadata": {},
   "source": [
    "First we need to verify that the raw PDFs exist locally, if not download and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b97cb9-98ff-475f-b983-b98e466897c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from openai import OpenAI\n",
    "\n",
    "# Directory to check for txt files exist\n",
    "txt_directory = '../data/scratch/txt/'\n",
    "\n",
    "# Check if the directory exists and contains any PDFs\n",
    "def check_txt_files_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        return False\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.lower().endswith('.txt'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_chat_response(prompt):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the chat model and returns the response.\n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The input prompt to send to the chat model.\n",
    "\n",
    "    Returns:\n",
    "    str: The response from the chat model.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv('NVCF_KEY')\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key not found. Please set the NVCF_KEY environment variable.\")\n",
    "\n",
    "    client = OpenAI(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        api_key=api_key\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"mistralai/mistral-large\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        top_p=1,\n",
    "        max_tokens=1024,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response += chunk.choices[0].delta.content\n",
    "\n",
    "    return response\n",
    "\n",
    "def summarize_text(text)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a highly advanced language model. Your task is to summarize the following document while retaining all critical information and context. The summary should include key points, important details, and any relevant context to ensure the summarized content is as useful and informative as the original document. Please focus on the main ideas, significant data, and essential concepts. Avoid unnecessary details and redundancy.\n",
    "\n",
    "    Document to Summarize:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "\n",
    "    response = get_chat_response(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63047f1e-a73c-45b2-bead-027a0adc3941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text files already exist in the directory.\n"
     ]
    }
   ],
   "source": [
    "# Path to the script to run if PDFs are not found\n",
    "script_path = 'scrape_mpep_from_web.py'\n",
    "\n",
    "# Check if PDFs exist, if not run the script\n",
    "if check_txt_files_exist(txt_directory):\n",
    "    print(\"Raw text files already exist in the directory.\")\n",
    "else:\n",
    "    print(\"Raw text files not found. Running the download script...\")\n",
    "    result = subprocess.run(['python3', script_path], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Script executed successfully.\")\n",
    "    else:\n",
    "        print(\"Error running the script.\")\n",
    "        print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eee3425-209d-4c25-8e32-bac6a13e1b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/workbench/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Directory containing the text files\n",
    "txt_directory = '../data/scratch/txt/'\n",
    "\n",
    "# Predefined token limit for chunking\n",
    "token_limit = 1000  # You can adjust this limit as needed\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the given text using NLTK's word_tokenize.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The text content to be tokenized.\n",
    "    \n",
    "    Returns:\n",
    "    int: The number of tokens in the text.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def should_chunk_file(file_path, token_limit):\n",
    "    \"\"\"\n",
    "    Determines if the file should be chunked based on the token count.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Path to the text file.\n",
    "    token_limit (int): The token limit for chunking.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Number of tokens and boolean indicating if chunking is needed.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        token_count = count_tokens(content)\n",
    "        return token_count, token_count > token_limit\n",
    "\n",
    "# List to store file data\n",
    "file_data = []\n",
    "\n",
    "# Iterate over each file in the directory and collect data\n",
    "for file_name in os.listdir(txt_directory):\n",
    "    file_path = os.path.join(txt_directory, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        token_count, needs_chunking = should_chunk_file(file_path, token_limit)\n",
    "        file_data.append({\n",
    "            \"filename\": file_name,\n",
    "            \"num_tokens\": token_count,\n",
    "            \"needs_chunking\": needs_chunking\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(file_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38efc9c9-ea57-441c-b054-82b1bc441827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>needs_chunking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s2263.txt</td>\n",
       "      <td>55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s2720.txt</td>\n",
       "      <td>2075</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s2262.txt</td>\n",
       "      <td>956</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s2272.txt</td>\n",
       "      <td>1535</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s2612.txt</td>\n",
       "      <td>71</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename  num_tokens  needs_chunking\n",
       "0  s2263.txt          55           False\n",
       "1  s2720.txt        2075            True\n",
       "2  s2262.txt         956           False\n",
       "3  s2272.txt        1535            True\n",
       "4  s2612.txt          71           False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15b4c605-ae96-4eda-ab8e-633f0c4bb91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80897"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num_tokens'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b6675-eb67-42d7-8909-6b00ec6607c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"Please correct the content provide, removing extra spaces, missing spaces, and other formatting errors. ONLY respond with the improved content, DO NOT include any additional text. Content: {pages[0]}\"\n",
    "response = get_chat_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37e92475-1ae1-451c-9fd2-e12bcaa6800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91702572-408f-46c6-93c4-5f27f8499f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader('../data/scratch/txt/', glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2a4fa21-cded-402f-acb6-9060c50fe554",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[1;32m      5\u001b[0m documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
