"""Wrapper around NeMo APIs."""
import logging
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Extra, root_validator

from langchain.llms.base import LLM
from langchain.llms.utils import enforce_stop_tokens
from langchain.utils import get_from_dict_or_env
from openai import OpenAI

logger = logging.getLogger(__name__)

class NemoLLMWrapper:
    """Wrapper around LLM functionality."""
    def __init__(self, nemo_api_key: str):
        self.client = OpenAI(
            base_url="https://integrate.api.nvidia.com/v1",
            api_key=nemo_api_key
        )
    
    def generate(self, model: str, prompt: str, **params):
        response = self.client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            **params
        )
        return ''.join(chunk.choices[0].delta.content for chunk in response if chunk.choices[0].delta.content)

class Nemo(BaseModel):
    """Wrapper around NeMo large language models.
    To use, you should have the ``openai`` python package installed, and the
    environment variable ``NEMO_API_KEY`` set with your API key
    """

    llm: LLM  #: :meta private:
    model: Optional[str] = None
    """Model name to use."""
    
    model_id: str = "mistralai/mistral-large"
    """Denotes the model type"""

    max_tokens: int = 512
    """Denotes the number of tokens to predict per generation."""

    temperature: float = 0.1
    """A non-negative float that tunes the degree of randomness in generation."""

    top_k: int = 5
    """Number of most likely tokens to consider at each step."""

    top_p: float = 1.0
    """Total probability mass of tokens to consider at each step."""

    repetition_penalty: float = 1.0
    """Penalizes repeated tokens according to frequency. Between 0 and 1."""

    length_penalty: float = 1.0
    """Penalizes repeated tokens. Between 0 and 1."""

    truncate: Optional[str] = None
    """Specify how the client handles inputs longer than the maximum token
    length: Truncate from START, END or NONE"""

    nemo_api_key: Optional[str] = None

    stop: Optional[List[str]] = None

    class Config:
        """Configuration for this pydantic object."""
        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that api key and python package exists in environment."""
        nemo_api_key = get_from_dict_or_env(
            values, "nemo_api_key", "NEMO_API_KEY"
        )
        values["llm"] = NemoLLMWrapper(nemo_api_key=nemo_api_key)
        return values

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling NEMO API."""
        return {
            "max_tokens": self.max_tokens,
            "temperature": self.temperature,
            "top_p": self.top_p,
        }

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {**{"model": self.model}, **self._default_params}

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """Call out to NeMo's generate_completion endpoint.
        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
        Returns:
            The string generated by the model.
        """
        params = self._default_params
        if self.stop is not None and stop is not None:
            raise ValueError("`stop` found in both the input and default params.")
        elif self.stop is not None:
            params["stop"] = self.stop
        else:
            params["stop"] = stop
        
        if self.model_id == "mistralai/mistral-large":
            prompt = prompt.split("Helpful Answer:")[0]
    
        text = self.llm.generate(model=self.model_id, prompt=prompt, **params)
        
        # If stop tokens are provided, enforce them.
        if stop is not None or self.stop is not None:
            text = enforce_stop_tokens(text, params["stop"])
        return text
